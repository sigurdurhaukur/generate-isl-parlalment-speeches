{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for continuing training of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 11:52:14.339032: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-17 11:52:14.700653: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 11:52:15.483714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_path = \"./models-that-work/ice-gpt2\"\n",
    "tokenizer_path = \"./models-that-work/ice-tokenizer\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# left padding\n",
    "tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/haukur/.cache/huggingface/datasets/mideind___json/mideind--icelandic-common-crawl-corpus-IC3-15f813c6a91f241e/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197a4c5dfc744834b9558229207b7451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/haukur/.cache/huggingface/datasets/mideind___json/mideind--icelandic-common-crawl-corpus-IC3-15f813c6a91f241e/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-6438ec45f6a1cab3.arrow\n",
      "Loading cached processed dataset at /home/haukur/.cache/huggingface/datasets/mideind___json/mideind--icelandic-common-crawl-corpus-IC3-15f813c6a91f241e/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-91a30a0b68c06d2a.arrow\n",
      "Loading cached processed dataset at /home/haukur/.cache/huggingface/datasets/mideind___json/mideind--icelandic-common-crawl-corpus-IC3-15f813c6a91f241e/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-8314f1c4b0df94ee.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_dataset = load_dataset(\"mideind/icelandic-common-crawl-corpus-IC3\")\n",
    "\n",
    "# filter out the empty strings\n",
    "huggingface_dataset = huggingface_dataset.filter(lambda example: len(example[\"text\"]) > 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(huggingface_dataset['train'][4][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69f118b1fd34e8aa9bd796c0b68e468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1378242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3624301552f04acbbe716bd2999b2f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/85931 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "context_length = 512\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_length=True,\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    # padding=True,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "\n",
    "# limit = 10000 # for debugging\n",
    "# ds_train = huggingface_dataset[\"train\"].select(range(int(limit)))\n",
    "# ds_test = huggingface_dataset[\"test\"].select(range(int(limit/10))))\n",
    "\n",
    "ds_train = huggingface_dataset[\"train\"]\n",
    "ds_test = huggingface_dataset[\"test\"]\n",
    "\n",
    "\n",
    "# tokenized_dataset = huggingface_dataset.map(tokenize_function, batched=True, batch_size=1000, num_proc=4, remove_columns=[\"text\"])\n",
    "tokanized_train_dataset = ds_train.map(tokenize_function, batched=True, batch_size=1000, num_proc=4, remove_columns=[\"text\"])\n",
    "tokanized_test_dataset = ds_test.map(tokenize_function, batched=True, batch_size=1000, num_proc=4, remove_columns=[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haukur/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10767' max='10767' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10767/10767 4:28:34, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.080900</td>\n",
       "      <td>0.994996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.016200</td>\n",
       "      <td>0.948333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10767, training_loss=1.044804362341965, metrics={'train_runtime': 16117.7994, 'train_samples_per_second': 85.511, 'train_steps_per_second': 0.668, 'total_flos': 4.507914432046694e+16, 'train_loss': 1.044804362341965, 'epoch': 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ice-gpt2\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokanized_train_dataset,\n",
    "    eval_dataset=tokanized_test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ice-gpt2-common-crawl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Það kviknaði í laugardalshöllini í gærmorgun og var eftir tveggja daga brota. Hann var á leið frá Laugavegi 17, en þetta var óvenju stútfullur af þessum toga. Stútfullir af ýmiskonar verkum, sem voru á leið frá Laugavegi 18, en þar var þó ekki leyfilegur. En það var ekki lengur spurningum þeirra sem voru á leiðinni frá Laugavegi 16, en það var þó ekki á leið frá Laugavegi 18, en það var þó ekki lengur sérstaklega á leiðinni. Mynd : Kjartan Guðmundsson. Við þurfum að hafa áhyggjur af því að það verði ekki lengur s\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Generate text\n",
    "# generated_text = pipeline(\"Fréttir\", max_length=512, num_return_sequences=1, repetition_penalty=1.2, top_k=50, top_p=0.95, temperature=1.1)\n",
    "# generated_text = pipeline(\"Það kviknaði í laugardalshöllini í gærmorgun\", max_length=512, num_return_sequences=1, repetition_penalty=1.3, top_k=50, top_p=0.95, temperature=1.1)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
